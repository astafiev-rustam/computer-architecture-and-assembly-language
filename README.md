|||
|---|---|
|ДИСЦИПЛИНА|Архитектура комьютера и язык ассемблера|
|ИНСТИТУТ|Передовая инженерная школа СВЧ-электроники|
|КАФЕДРА|Передовых технологий|
|ВИД УЧЕБНОГО МАТЕРИАЛА|Методические указания по дисциплине|
|ПРЕПОДАВАТЕЛЬ|Астафьев Рустам Уралович|
|СЕМЕСТР|1 семестр, 2025/2026 уч. год|

Ссылка на материал: <br>
https://github.com/astafiev-rustam/computer-architecture-and-assembly-language/tree/practice-1-1

# Практическое занятие №1: Информация и информатика

Теория информации, фундамент которой заложил Клод Шеннон в середине XX века, возникла из прагматичной необходимости оптимизации систем связи. Её ключевая идея — **информация есть мера уменьшения неопределенности (энтропии)**. Это означает, что мы измеряем не смысл сообщения, а то, насколько оно делает наше знание более определенным. Сообщение о случайном и маловероятном событии (например, «в Москве в июле пошел снег») несет гораздо больше информации, чем сообщение о событии ожидаемом («в Москве в июле тепло»).

## **Термины и определения:**

**Информация (в техническом смысле)** — это снятая неопределенность. Количество информации измеряется изменением энтропии системы.

**Энтропия (H)** — мера неопределенности или хаотичности системы. В теории информации это средняя мера количества информации, приходящейся на одно сообщение из источника. Чем выше энтропия, тем более непредсказуемы сообщения источника и тем больше информации несет каждое из них.

**Бит** — базовая единица измерения информации, определяющая количество информации, содержащейся в сообщении, которое уменьшает неопределенность ровно в два раза (выбор из двух равновероятных событий).

**Вероятностный подход** — подход к измерению информации, основанный на использовании понятий теории вероятностей. Количество информации в сообщении обратно пропорционально вероятности его появления.

**Алфавитный подход** — подход, при котором количество информации оценивается по длине кода (количеству символов), необходимого для её представления, без учета смысла.

## **Формулы измерения информации**

**Количество информации для отдельного события.**
    Если событие имеет вероятность `p`, то количество информации `i`, содержащееся в сообщении о его наступлении, вычисляется по формуле Хартли в логарифмическом виде:
    `i = log₂(1/p) = -log₂(p)` (бит).

*Пример: Результат подбрасывания идеальной монеты (орел или решка) имеет вероятность p=0.5. Количество информации в сообщении о результате равно i = -log₂(0.5) = 1 бит.*

**Энтропия источника информации (средняя информация).**
Для источника, генерирующего множество событий (символов) с вероятностями `p₁, p₂, ..., pₙ`, энтропия `H` (среднее количество информации на одно сообщение) рассчитывается как сумма:
`H = - Σ (pᵢ * log₂(pᵢ))`, где суммирование ведется от `i=1` до `n`.

*Пример: Рассмотрим источник, генерирующий два символа: 'A' (p=0.75) и 'B' (p=0.25). Его энтропия: H = - (0.75 * log₂(0.75) + 0.25 * log₂(0.25)) ≈ 0.81 бит/символ. Это меньше 1 бита, так как символ 'A' предсказуем.*

## **Место двоичной системы счисления**

Абстрактное понятие бита находит свое идеальное физическое воплощение в двоичной системе счисления. Её доминирование в цифровой технике обусловлено следующими ключевыми факторами:

**Технологическая надежность.** Проще и дешевле создавать электронные элементы, которые надежно работают в двух состояниях (транзистор «открыт/закрыт», напряжение «высокое/низкое», магнитный домен «намагничен/размагничен»), чем в десяти и более. Распознавание двух состояний значительно устойчивее к помехам.

**Логическая интерпретация.** Двоичная система напрямую соответствует булевой алгебре, оперирующей значениями «ИСТИНА» (1) и «ЛОЖЬ» (0). Это позволяет унифицировать арифметические и логические операции внутри процессора.

**Универсальность представления.** Любая информация — числа, текст, команды, мультимедиа — может быть закодирована в виде последовательности битов.

## **Трактовка вычислительных процессов**

С позиций теории информации, работа любого вычислительного устройства — это не что иное, как **последовательное преобразование информационной энтропии**. Исходные данные обладают высокой неопределенностью для пользователя. Процессор, выполняя детерминированную последовательность логических и арифметических операций над битами, преобразует эти данные в результат, который имеет для пользователя низкую энтропию (т.е. является решением задачи). Таким образом, вычисление — это целенаправленное уменьшение неопределенности входных данных по строго определенным законам, реализованным в виде физических процессов в электронных схемах. Эта трактовка связывает абстрактную математику Шеннона с конкретной инженерией, показывая, что архитектура компьютера — это материальная реализация принципов управления информацией.
## Работа с Logisim

Основной платформой реализации практических работ по дисциплине будет являться среда Logisim, которая представляет собой среду моделирования логических схем и структур на базовых элементах логики, позволяет моделировать поведение реальных построенных объектов, изучать на них основные принципы их работы, в т.ч. проектировать основные логические автоматы и структуры.

Загрузить приложение на свой компьютер можно здесь:
https://cburch.com/logisim/ru/download.html


После успешной установке рекомендуется ознакомиться с инструкциями по работе с платформой и документацией по ней:
https://cburch.com/logisim/docs/2.7/ru/html/guide/tutorial/index.html